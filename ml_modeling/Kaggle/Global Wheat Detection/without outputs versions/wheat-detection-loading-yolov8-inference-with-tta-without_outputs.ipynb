{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import cv2\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available devices: \\n\")\n",
    "for device in tf.config.list_logical_devices():\n",
    "    print(device.name, device.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "    \"\"\"\n",
    "    Detects and returns the best TensorFlow distribution strategy.\n",
    "    - TPUStrategy for TPU(s)\n",
    "    - MirroredStrategy for GPU(s)\n",
    "    - Default strategy for CPU\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try TPU first\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print(\"Using TPU strategy:\", type(strategy).__name__)\n",
    "    except Exception:\n",
    "        # If TPU not available, try GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(\"Using GPU strategy:\", type(strategy).__name__)\n",
    "        else:\n",
    "            # Fallback CPU\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            print(\"No TPU/GPU found. Using CPU strategy:\", type(strategy).__name__)\n",
    "\n",
    "    print(\"REPLICAS:\", strategy.num_replicas_in_sync)\n",
    "    return strategy\n",
    "\n",
    "# Call it\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (1024, 1024)\n",
    "NUM_CLASSES = 1\n",
    "NUM_TTA = 8\n",
    "CONF_THRESHOLD = 0.2\n",
    "IOU_THRESHOLD = 0.5\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE_PER_REPLICA = 1\n",
    "BUFFER_SHUFFLE_SIZE = 512\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "print(f'Global Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/global-wheat-detection'\n",
    "MODEL_DIR = '/kaggle/input/wheat-detection/keras/warmup/3/finetune_best_model.keras'\n",
    "PACKAGE_DIR = '/kaggle/input/wheat-detection-ensemble-boxes/isolated_ensemble_boxes/'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PACKAGE_DIR not in sys.path:\n",
    "    sys.path.append(PACKAGE_DIR)\n",
    "\n",
    "print(f\"Added isolated package path to sys.path: {PACKAGE_DIR}\")\n",
    "\n",
    "import ensemble_boxes\n",
    "print(\"✅ ensemble_boxes imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from ensemble_boxes import weighted_boxes_fusion\n",
    "except ImportError:\n",
    "    print(\"WARNING: The 'ensemble_boxes' library is required for WBF.\")\n",
    "    print(\"Please run: pip install ensemble-boxes\")\n",
    "    # Placeholder to prevent crash, you must install the library to proceed\n",
    "    weighted_boxes_fusion = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    print(\"Loading model ...\")\n",
    "    yolo_model = tf.keras.models.load_model(MODEL_DIR,\n",
    "            custom_objects = {\n",
    "                'YOLOV8Detector': keras_cv.models.YOLOV8Detector,\n",
    "                'YOLOV8Backbone': keras_cv.models.YOLOV8Backbone\n",
    "            }\n",
    "    )\n",
    "    print(\"Model loaded successfully. Ready for Visualization and Test Time !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "# Convert the string representation of the list in 'bbox' column to an actual list\n",
    "df['bbox'] = df['bbox'].apply(ast.literal_eval)\n",
    "# Extract coordinates from the list: [x_min, y_min, x_max, y_max]\n",
    "df['x_min'] = df['bbox'].apply(lambda b: b[0])\n",
    "df['y_min'] = df['bbox'].apply(lambda b: b[1])\n",
    "df['x_max'] = df['bbox'].apply(lambda b: b[0] + b[2])\n",
    "df['y_max'] = df['bbox'].apply(lambda b: b[1] + b[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_with_bboxes(df, image_dir, nrows, ncols):\n",
    "    # Pick random images from the train dir\n",
    "    files = os.listdir(image_dir)\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "\n",
    "    for ax, fname in zip(axs.flatten(), files):\n",
    "        image_id = fname.replace('.jpg', '')\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get bboxes if exists\n",
    "        if image_id in df['image_id'].values:\n",
    "            bboxes = df[df['image_id'] == image_id][['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "            for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "                start_point = (int(x_min), int(y_min))\n",
    "                end_point = (int(x_max), int(y_max))\n",
    "                color = (255, 0, 0)\n",
    "                thickness = 2\n",
    "                cv2.rectangle(img, start_point, end_point, color, thickness)\n",
    "\n",
    "        # Show image\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(fname, fontsize=8)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_with_bboxes(df, TRAIN_DIR, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_image(image_path):\n",
    "    \"\"\"Loads and resizes a single image for prediction.\"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detections(model, image_path, bounding_box_format=\"xyxy\", \n",
    "                         confidence_threshold=CONF_THRESHOLD):\n",
    "    files = os.listdir(image_path)[:4]\n",
    "    paths = [os.path.join(image_path, f) for f in files]\n",
    "    num_images = len(paths)\n",
    "\n",
    "    image_list = [load_single_image(path) for path in paths]\n",
    "    images = tf.stack(image_list)\n",
    "    print(f\"Loaded {num_images} images into a batch of shape {images.shape}\")\n",
    "    \n",
    "    # Run model inference on the batch\n",
    "    y_pred = model.predict(images)\n",
    "\n",
    "    # y_pred is a dictionary: {'boxes': ..., 'confidence': ..., 'classes': ...}\n",
    "    # Filter low-confidence boxes manually (since keras_cv.bounding_box has no filter)\n",
    "    conf_mask = y_pred[\"confidence\"] > confidence_threshold\n",
    "\n",
    "    # Create filtered prediction dict\n",
    "    y_pred_filtered = {\n",
    "        \"boxes\": tf.ragged.boolean_mask(y_pred[\"boxes\"], conf_mask),\n",
    "        \"classes\": tf.ragged.boolean_mask(y_pred[\"classes\"], conf_mask),\n",
    "        \"confidence\": tf.ragged.boolean_mask(y_pred[\"confidence\"], conf_mask),\n",
    "    }\n",
    "\n",
    "    # Visualize with KerasCV utility\n",
    "    keras_cv.visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_pred=y_pred_filtered,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "    )\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = detections(yolo_model, TRAIN_DIR, bounding_box_format=\"xyxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = detections(yolo_model, TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_h_flip(boxes, image_width= IMG_SIZE[1]):\n",
    "    \"\"\"Reverses a horizontal flip for pixel-space [xmin, ymin, xmax, ymax] boxes.\"\"\"\n",
    "    xmin = image_width - boxes[:, 2]\n",
    "    ymin = boxes[:, 1]\n",
    "    xmax = image_width - boxes[:, 0]\n",
    "    ymax = boxes[:, 3]\n",
    "    return tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_v_flip(boxes, image_height= IMG_SIZE[0]):\n",
    "    \"\"\"Reverses a vertical flip for normalized [xmin, ymin, xmax, ymax] boxes.\"\"\"\n",
    "    # ymin_new = 1 - ymax_old; ymax_new = 1 - ymin_old\n",
    "    xmin = boxes[:, 0]     # Use xmin index (0)\n",
    "    ymin = image_height - boxes[:, 3] # Use ymax index (3)\n",
    "    xmax = boxes[:, 2]     # Use xmax index (2)\n",
    "    ymax = image_height - boxes[:, 1] # Use ymin index (1)\n",
    "    \n",
    "    # Must stack in the original [xmin, ymin, xmax, ymax] order\n",
    "    return tf.stack([xmin, ymin, xmax, ymax], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply multiple, slightly random color/light transforms\n",
    "def random_color_jitter(img):\n",
    "    # Apply small random hue shift\n",
    "    img = tf.image.random_hue(img, max_delta=0.04) \n",
    "    # Apply small random contrast\n",
    "    img = tf.image.random_contrast(img, lower=0.85, upper=1.15) \n",
    "    # Apply small random saturation\n",
    "    img = tf.image.random_saturation(img, lower=0.85, upper=1.15)\n",
    "    # Apply small random brightness\n",
    "    img = tf.image.random_brightness(img, max_delta=0.04)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = os.path.join(TEST_DIR, os.listdir(TEST_DIR)[3])\n",
    "test_img = load_single_image(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_filter(model, image_tensor, conf_threshold):\n",
    "    \"\"\"\n",
    "    Runs model prediction on a single image, filters results by confidence.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Keras model.\n",
    "        image_tensor: The input image (H, W, 3).\n",
    "        conf_threshold: Minimum confidence score to keep a detection.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of filtered predictions {\"boxes\", \"classes\", \"confidence\"}\n",
    "              in KerasCV RaggedTensor format, with batch dimension [1, None, ...].\n",
    "    \"\"\"\n",
    "    # Add batch dimension: [1, H, W, 3]\n",
    "    input_tensor = tf.expand_dims(image_tensor, 0)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(input_tensor, verbose=0)\n",
    "\n",
    "    # Filter by confidence\n",
    "    conf_mask = y_pred[\"confidence\"] > conf_threshold\n",
    "    \n",
    "    # Apply mask to all prediction components\n",
    "    y_pred_filtered = {\n",
    "        \"boxes\": tf.ragged.boolean_mask(y_pred[\"boxes\"], conf_mask),\n",
    "        \"classes\": tf.ragged.boolean_mask(y_pred[\"classes\"], conf_mask),\n",
    "        \"confidence\": tf.ragged.boolean_mask(y_pred[\"confidence\"], conf_mask),\n",
    "    }\n",
    "    \n",
    "    return y_pred_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(image_tensor, predictions, title, **kwargs):\n",
    "    \"\"\"\n",
    "    Visualizes the prediction on the given image tensor.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: The image to plot on (H, W, 3).\n",
    "        predictions: The prediction dictionary from predict_and_filter.\n",
    "        title: Title to print before visualization.\n",
    "        **kwargs: Optional keyword arguments for plot_bounding_box_gallery.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    \n",
    "    keras_cv.visualization.plot_bounding_box_gallery(\n",
    "        tf.expand_dims(image_tensor, 0),\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=\"xyxy\",\n",
    "        y_pred=predictions,\n",
    "        scale=4,\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        **kwargs\n",
    "    )\n",
    "    print(f\"✅ {title} visualized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Original Image Prediction ---\n",
    "y_pred_original = predict_and_filter(\n",
    "    yolo_model, \n",
    "    test_img, \n",
    "    CONF_THRESHOLD\n",
    ")\n",
    "\n",
    "visualize_prediction(\n",
    "    test_img, \n",
    "    y_pred_original, \n",
    "    \"Original Prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Vertical Flip, Predict, and Visualize Flipped ---\n",
    "vflipped_img = tf.image.flip_up_down(test_img)\n",
    "\n",
    "y_pred_flip = predict_and_filter(\n",
    "    yolo_model, \n",
    "    vflipped_img, \n",
    "    CONF_THRESHOLD\n",
    ")\n",
    "\n",
    "visualize_prediction(\n",
    "    vflipped_img, \n",
    "    y_pred_flip, \n",
    "    \"Flipped Image Prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: Reverse Boxes and Visualize on Original Image ---\n",
    "print(\"\\nReversing flipped boxes to original coordinates...\")\n",
    "\n",
    "# Extract boxes from the filtered RaggedTensor (get the dense tensor at index 0)\n",
    "boxes_augmented = y_pred_flip[\"boxes\"][0] \n",
    "\n",
    "# Note: Your original reverse_v_flip function requires the image dimension\n",
    "# We assume it takes IMG_SIZE[0] (1024) as the image_height argument.\n",
    "# We must ensure reverse_v_flip is passed the image size as defined in your setup.\n",
    "boxes_reversed = reverse_v_flip(boxes_augmented)\n",
    "\n",
    "# Reformat the reversed boxes into the prediction dictionary for plotting\n",
    "y_pred_reversed = {\n",
    "    \"boxes\": tf.RaggedTensor.from_tensor(tf.expand_dims(boxes_reversed, axis=0)),\n",
    "    \"classes\": y_pred_flip[\"classes\"],\n",
    "    \"confidence\": y_pred_flip[\"confidence\"],\n",
    "}\n",
    "\n",
    "visualize_prediction(\n",
    "    test_img, \n",
    "    y_pred_reversed, \n",
    "    \"Reversed Flipped Boxes on Original Image\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TTA Sequence Definition (for [xmin, ymin, xmax, ymax] format) ---\n",
    "TTA_SEQUENCES = [\n",
    "    # 0. Baseline (Identity)\n",
    "    (lambda img: img, lambda boxes: boxes),  \n",
    "    \n",
    "    # 1. Horizontal Flip\n",
    "    (tf.image.flip_left_right, reverse_h_flip),\n",
    "    \n",
    "    # 2. Vertical Flip\n",
    "    (tf.image.flip_up_down, reverse_v_flip),\n",
    "    \n",
    "    # 3-6. Four different, random photometric variants (No box reversal needed)\n",
    "    (lambda img: random_color_jitter(img), lambda boxes: boxes), \n",
    "    (lambda img: random_color_jitter(img), lambda boxes: boxes), \n",
    "    (lambda img: random_color_jitter(img), lambda boxes: boxes), \n",
    "    (lambda img: random_color_jitter(img), lambda boxes: boxes), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(model, image_tensor, tta_sequences, conf_threshold):\n",
    "    \"\"\"\n",
    "    Applies TTA sequences, collects predictions, reverses boxes, and aggregates results \n",
    "    for a single image. **Note: Relies on external global variable IMG_SIZE[0] for dimensions\n",
    "    if not passed to reverse_boxes_fn.**\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model.\n",
    "        image_tensor: The input image tensor (e.g., test_img, shape [H, W, 3]).\n",
    "        tta_sequences: The list of (augment_fn, reverse_fn) tuples.\n",
    "        conf_threshold: The minimum confidence for filtering individual TTA predictions.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_boxes_np, all_confidences_np, all_classes_np) as concatenated NumPy arrays.\n",
    "    \"\"\"\n",
    "    all_tta_predictions = []\n",
    "\n",
    "    print(f\"--- Running TTA for 1 image ({len(tta_sequences)} sequences) ---\")\n",
    "\n",
    "    for i, (augment_image_fn, reverse_boxes_fn) in enumerate(TTA_SEQUENCES):\n",
    "        # 1. Augment and Prepare Input\n",
    "        augmented_img = augment_image_fn(image_tensor)\n",
    "        input_tensor = tf.expand_dims(augmented_img, 0)\n",
    "        \n",
    "        # 2. Predict (verbose=0 suppresses Keras logging)\n",
    "        y_pred = model.predict(input_tensor, verbose=0)\n",
    "\n",
    "        # 3. Filter by confidence\n",
    "        conf_mask = y_pred[\"confidence\"] > conf_threshold\n",
    "        \n",
    "        # Extract and un-batch filtered tensors\n",
    "        boxes_augmented = tf.ragged.boolean_mask(y_pred[\"boxes\"], conf_mask)[0]\n",
    "        classes = tf.ragged.boolean_mask(y_pred[\"classes\"], conf_mask)[0]\n",
    "        confidence = tf.ragged.boolean_mask(y_pred[\"confidence\"], conf_mask)[0]\n",
    "\n",
    "        n_detections = tf.shape(boxes_augmented)[0].numpy()\n",
    "        \n",
    "        if n_detections > 0:\n",
    "            # Note: We must now pass the image_dim back, as reverse functions typically need it\n",
    "            # unless they were hard-coded to 1024 (which is poor practice).\n",
    "            # Assuming reverse_boxes_fn still expects the dimension for safety:\n",
    "            boxes_reversed = reverse_boxes_fn(boxes_augmented) \n",
    "            all_tta_predictions.append((boxes_reversed, confidence, classes))\n",
    "        \n",
    "        # New: Log every single TTA sequence\n",
    "        print(f\"  > Sequence {i+1}/{len(tta_sequences)} ({n_detections} boxes) processed.\")\n",
    "\n",
    "\n",
    "    if not all_tta_predictions:\n",
    "        print(\"--- Aggregation: No detections found above confidence threshold. ---\")\n",
    "        return None, None, None\n",
    "\n",
    "    # 4. Aggregate all predictions into single NumPy arrays\n",
    "    list_of_boxes = [p[0].numpy() for p in all_tta_predictions]\n",
    "    list_of_confidences = [p[1].numpy() for p in all_tta_predictions]\n",
    "    list_of_classes = [p[2].numpy() for p in all_tta_predictions]\n",
    "\n",
    "    all_boxes_np = np.concatenate(list_of_boxes, axis=0)\n",
    "    all_confidences_np = np.concatenate(list_of_confidences, axis=0)\n",
    "    # Corrected potential bug: Use list_of_classes instead of all_classes_np\n",
    "    all_classes_np = np.concatenate(list_of_classes, axis=0)\n",
    "\n",
    "    print(f\"--- Aggregation Complete. Total boxes for WBF: {len(all_boxes_np)} ---\")\n",
    "    \n",
    "    return all_boxes_np, all_confidences_np, all_classes_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes_np, all_confidences_np, all_classes_np = tta(yolo_model, test_img, \n",
    "                                                       TTA_SEQUENCES, CONF_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wbf(all_boxes_np, all_confidences_np, \n",
    "        all_classes_np, img_size, conf_threshold, iou_threshold):\n",
    "    \"\"\"\n",
    "    Performs WBF on aggregated predictions and converts results to Kaggle submission string.\n",
    "\n",
    "    Returns:\n",
    "        str: Space-delimited prediction string in \"conf x y w h\" format.\n",
    "        dict: Fused prediction dictionary for visualization (KerasCV format).\n",
    "    \"\"\"\n",
    "    image_dim = img_size[0]\n",
    "\n",
    "    # 1. Normalize pixel coordinates to [0.0, 1.0] for WBF\n",
    "    normalized_boxes = all_boxes_np / image_dim\n",
    "    \n",
    "    # WBF expects list of lists\n",
    "    boxes = [normalized_boxes.tolist()]\n",
    "    scores = [all_confidences_np.tolist()]\n",
    "    labels = [all_classes_np.astype(float).tolist()] \n",
    "\n",
    "    # 2. Apply Weighted Boxes Fusion (WBF)\n",
    "    fused_boxes_norm, fused_scores, fused_labels = weighted_boxes_fusion(\n",
    "        boxes,\n",
    "        scores,\n",
    "        labels,\n",
    "        weights=None,\n",
    "        iou_thr=iou_threshold,\n",
    "        conf_type='max',\n",
    "        skip_box_thr=0.0001\n",
    "    )\n",
    "    print(f\"--- WBF Applied. Boxes reduced to {len(fused_boxes_norm)} before final filter. ---\")\n",
    "    \n",
    "    # 3. Final Filtering and Denormalization\n",
    "    final_mask = fused_scores >= conf_threshold\n",
    "    final_boxes_norm = fused_boxes_norm[final_mask]\n",
    "    final_scores = fused_scores[final_mask]\n",
    "    final_labels_filtered = fused_labels[final_mask] # Capture filtered labels\n",
    "    \n",
    "    # Denormalize boxes back to [0, 1024] pixel space (xyxy format)\n",
    "    final_boxes_denorm_xyxy = final_boxes_norm * image_dim\n",
    "    \n",
    "    print(f\"--- Final {len(final_boxes_denorm_xyxy)} boxes remain after final confidence filter. ---\")\n",
    "\n",
    "    # 4. Format Conversion to Kaggle \"conf x y w h\"\n",
    "    prediction_strings = []\n",
    "    \n",
    "    # Iterate over the fused, filtered results\n",
    "    for box, score in zip(final_boxes_denorm_xyxy, final_scores):\n",
    "        # Convert to integer pixel values (Kaggle standard)\n",
    "        xmin, ymin, xmax, ymax = box.astype(np.int32)\n",
    "        \n",
    "        # Convert xyxy to xywh\n",
    "        w = xmax - xmin\n",
    "        h = ymax - ymin\n",
    "        \n",
    "        # Format: confidence xmin ymin w h (space-delimited)\n",
    "        box_string = f\"{score:.4f} {xmin} {ymin} {w} {h}\"\n",
    "        prediction_strings.append(box_string)\n",
    "        \n",
    "    submission_string = \" \".join(prediction_strings)\n",
    "    \n",
    "    # Optional: Show example submission string\n",
    "    if submission_string:\n",
    "        print(f\"--- Submission String Example: {submission_string[:100]}... ---\")\n",
    "    else:\n",
    "        print(\"--- Submission String is empty (No confident detections). ---\")\n",
    "    \n",
    "    # 5. Prepare KerasCV format for optional visualization\n",
    "    final_boxes_denorm_batched = tf.expand_dims(\n",
    "        tf.convert_to_tensor(final_boxes_denorm_xyxy, dtype=tf.float32), axis=0\n",
    "    )\n",
    "    final_scores_batched = tf.expand_dims(\n",
    "        tf.convert_to_tensor(final_scores, dtype=tf.float32), axis=0\n",
    "    )\n",
    "    \n",
    "    y_pred_tta_fused = {\n",
    "        \"boxes\": tf.RaggedTensor.from_tensor(final_boxes_denorm_batched),\n",
    "        # Use the filtered labels, converted to int32, and batched\n",
    "        \"classes\": tf.RaggedTensor.from_tensor(\n",
    "            tf.expand_dims(tf.convert_to_tensor(final_labels_filtered, dtype=tf.int32), axis=0)\n",
    "        ),\n",
    "        \"confidence\": tf.RaggedTensor.from_tensor(final_scores_batched),\n",
    "    }\n",
    "\n",
    "    return submission_string, y_pred_tta_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_string, y_pred_fused = wbf(all_boxes_np, \n",
    "                                      all_confidences_np, \n",
    "                                      all_classes_np,\n",
    "                                      IMG_SIZE,\n",
    "                                      CONF_THRESHOLD,\n",
    "                                      IOU_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the fused result from wbf_and_format:\n",
    "# submission_string, y_pred_fused = wbf_and_format(...)\n",
    "\n",
    "visualize_prediction(\n",
    "    image_tensor=test_img, \n",
    "    predictions=y_pred_fused, \n",
    "    title=\"Final WBF-Fused TTA Result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(model, test_ds, tta_sequences, img_size, conf_threshold, iou_threshold):\n",
    "    \"\"\"\n",
    "    Processes the entire test dataset using TTA/WBF and creates the submission DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model.\n",
    "        test_ds: The tf.data.Dataset for the test images.\n",
    "        ... all other necessary parameters\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The final submission dataframe.\n",
    "    \"\"\"\n",
    "    submission_data = {\"image_id\": [], \"PredictionString\": []}\n",
    "    \n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\">>> Starting Inference on Test Dataset with TTA/WBF <<<\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    # Iterate through the test dataset\n",
    "    for step, (images, image_ids) in enumerate(test_ds):\n",
    "        \n",
    "        # Loop through images in the current batch (usually batch_size=1 for TTA)\n",
    "        for image_tensor, image_id in zip(images, image_ids):\n",
    "            \n",
    "            image_id_str = image_id.numpy().decode('utf-8')\n",
    "            print(f\"\\n[IMAGE {step+1}] Processing image: {image_id_str}\")\n",
    "\n",
    "            # 1. Run TTA and Aggregate\n",
    "            all_boxes, all_confidences, all_classes = tta(\n",
    "                model, image_tensor, tta_sequences, conf_threshold\n",
    "            )\n",
    "            \n",
    "            if all_boxes is None:\n",
    "                # No detections, prediction string is empty\n",
    "                submission_string = \"\"\n",
    "                y_pred_fused = None\n",
    "            else:\n",
    "                # 2. Run WBF and Format\n",
    "                submission_string, y_pred_fused = wbf(\n",
    "                    all_boxes, all_confidences, all_classes, \n",
    "                    img_size, conf_threshold, iou_threshold\n",
    "                )\n",
    "            \n",
    "            # 3. Collect Result\n",
    "            submission_data[\"image_id\"].append(image_id_str)\n",
    "            submission_data[\"PredictionString\"].append(submission_string)\n",
    "\n",
    "            # 4. Optional: Visualize the first image result\n",
    "            if step == 1 and y_pred_fused is not None:\n",
    "                visualize_prediction(image_tensor, \n",
    "                                     predictions=y_pred_fused, \n",
    "                                     title=\"Final WBF-Fused TTA Result\")\n",
    "\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\">>> TTA/WBF Inference Complete. Creating Submission File <<<\")\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_inference(image_path):\n",
    "    \"\"\"Loads and resizes a single image for model prediction.\"\"\"\n",
    "    image = load_single_test_image(image_path)\n",
    "    image_id = tf.strings.split(image_path, os.sep)[-1]       # filename\n",
    "    image_id = tf.strings.regex_replace(image_id, \".jpg$\", \"\") # remove extension\n",
    "    return image, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(TEST_DIR):\n",
    "    test_images_path = [\n",
    "        os.path.join(TEST_DIR, fname) for fname in os.listdir(TEST_DIR)\n",
    "    ]\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(test_images_path)\n",
    "    test_ds = (test_ds.map(preprocess_for_inference, num_parallel_calls= AUTO)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      .prefetch(AUTO))\n",
    "    \n",
    "    print('Test Dataset created successfully !')\n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = create_submission_file(yolo_model,\n",
    "                       test_ds,\n",
    "                       TTA_SEQUENCES,\n",
    "                       IMG_SIZE,\n",
    "                       CONF_THRESHOLD,\n",
    "                       IOU_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission file created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1160143,
     "isSourceIdPinned": false,
     "sourceId": 19989,
     "sourceType": "competition"
    },
    {
     "sourceId": 278819654,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 483974,
     "modelInstanceId": 468144,
     "sourceId": 628693,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
