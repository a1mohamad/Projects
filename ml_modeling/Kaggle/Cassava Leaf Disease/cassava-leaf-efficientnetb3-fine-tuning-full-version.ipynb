{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":13836,"databundleVersionId":1718836,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cassava Leaf Disease Classification using EfficientNetB3\n\n## ğŸ“– Introduction\n\n**Cassava** is a critical staple crop for millions of people in Africa, Asia, and Latin America, providing a vital source of calories. However, its cultivation is severely threatened by various viral diseases, which can lead to significant yield losses and jeopardize food security. Accurate and early detection of these diseases is crucial for effective management and control.\n\nThis notebook tackles the **Cassava Leaf Disease Classification** challenge. The primary goal is to develop a machine learning model capable of accurately identifying the type of disease present on a cassava plant, or classifying it as healthy, based on an image of its leaf.\n\n### ğŸŒ¿ The Diseases\nThe dataset focuses on the following five categories:\n1.  **Cassava Bacterial Blight (CBB)**\n2.  **Cassava Brown Streak Disease (CBSD)**\n3.  **Cassava Green Mottle (CGM)**\n4.  **Cassava Mosaic Disease (CMD)**\n5.  **Healthy**\n\n### ğŸ¯ Our Approach\nWe will employ a powerful deep learning technique called **transfer learning**. Specifically, we will fine-tune a pre-trained **EfficientNetB3** model, a state-of-the-art convolutional neural network (CNN), on the cassava leaf dataset. To accelerate the training process, we'll leverage Google's Tensor Processing Units (TPUs). The workflow includes:\n\n- **Setting up the Environment**: Configuring the TPU strategy for distributed training.\n- **Data Preprocessing**: Creating an efficient data pipeline using TFRecords.\n- **Data Augmentation**: Applying various image transformations to enhance model robustness and prevent overfitting.\n- **Model Building**: Constructing a custom classifier on top of the EfficientNetB3 base.\n- **Two-Phase Training**: \n    1. Training only the classifier head.\n    2. Fine-tuning the entire model with a low learning rate.\n- **Inference**: Generating predictions on the test set.","metadata":{"execution":{"iopub.status.busy":"2025-08-16T18:14:05.588740Z","iopub.execute_input":"2025-08-16T18:14:05.589077Z","iopub.status.idle":"2025-08-16T18:14:05.606909Z","shell.execute_reply.started":"2025-08-16T18:14:05.589051Z","shell.execute_reply":"2025-08-16T18:14:05.601112Z"}}},{"cell_type":"markdown","source":"## ğŸ› ï¸ 1. Initial Setup and Imports\n\nLet's begin by importing the essential libraries for our project. We'll need `numpy` and `pandas` for data manipulation, `tensorflow` for building and training our deep learning model, `cv2` and `matplotlib` for image processing and visualization, and `os` and `json` for file handling.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T12:41:16.458235Z","iopub.execute_input":"2025-08-07T12:41:16.458769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport seaborn as sns\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:27:43.348176Z","iopub.execute_input":"2025-08-16T17:27:43.348412Z","iopub.status.idle":"2025-08-16T17:27:43.359457Z","shell.execute_reply.started":"2025-08-16T17:27:43.348391Z","shell.execute_reply":"2025-08-16T17:27:43.355472Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## ğŸŒ± 2. Ensuring Reproducibility\n\nTo make our experiments reproducible, it's crucial to set a global seed. This ensures that any process involving randomnessâ€”such as weight initialization, data shuffling, and augmentationâ€”produces the same results every time the code is run. The function below sets the seed for Python's `random` module, `NumPy`, and `TensorFlow`.","metadata":{}},{"cell_type":"code","source":"def seed_everthing(SEED=28):\n    # Set the Python's random module seed\n    random.seed(SEED)\n    \n    # Set the NumPy random seed\n    np.random.seed(SEED)\n    \n    # Set the TensorFlow random seed\n    tf.random.set_seed(SEED)\n\n    print(f\"Global seed set to {SEED} ğŸŒ±\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:27:43.368906Z","iopub.execute_input":"2025-08-16T17:27:43.369363Z","iopub.status.idle":"2025-08-16T17:27:43.377825Z","shell.execute_reply.started":"2025-08-16T17:27:43.369339Z","shell.execute_reply":"2025-08-16T17:27:43.373760Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"seed_everthing()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:29:11.801147Z","iopub.execute_input":"2025-08-16T17:29:11.801483Z","iopub.status.idle":"2025-08-16T17:29:11.812029Z","shell.execute_reply.started":"2025-08-16T17:29:11.801456Z","shell.execute_reply":"2025-08-16T17:29:11.806440Z"}},"outputs":[{"name":"stdout","text":"Global seed set to 28 ğŸŒ±\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## ğŸš€ 3. Hardware Accelerator Configuration\n\nTraining deep learning models can be computationally intensive. To speed up this process, we'll use a hardware accelerator like a TPU (Tensor Processing Unit) or GPU (Graphics Processing Unit). The following code detects the available hardware and sets up the appropriate TensorFlow distribution strategy. \n\n- **TPUStrategy**: For training on TPUs.\n- **MirroredStrategy**: For training on multiple GPUs on a single machine.\n- **Default Strategy**: For training on a single GPU or CPU.\n\nThe number of `REPLICAS` indicates how many parallel processing units are available, which is essential for scaling our batch size and distributing the training workload.","metadata":{}},{"cell_type":"code","source":"print(\"Available devices:\")\nfor device in tf.config.list_logical_devices():\n    print(device.name, device.device_type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:29:12.293336Z","iopub.execute_input":"2025-08-16T17:29:12.293577Z","iopub.status.idle":"2025-08-16T17:29:16.209360Z","shell.execute_reply.started":"2025-08-16T17:29:12.293556Z","shell.execute_reply":"2025-08-16T17:29:16.203641Z"}},"outputs":[{"name":"stdout","text":"Available devices:\n/device:CPU:0 CPU\n/device:TPU:0 TPU\n/device:TPU:1 TPU\n/device:TPU:2 TPU\n/device:TPU:3 TPU\n/device:TPU:4 TPU\n/device:TPU:5 TPU\n/device:TPU:6 TPU\n/device:TPU:7 TPU\n/device:TPU_SYSTEM:0 TPU_SYSTEM\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1755365356.181975      10 service.cc:148] XLA service 0x5a29267da9d0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1755365356.182024      10 service.cc:156]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1755365356.182028      10 service.cc:156]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1755365356.182031      10 service.cc:156]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1755365356.182034      10 service.cc:156]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1755365356.182036      10 service.cc:156]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1755365356.182039      10 service.cc:156]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1755365356.182042      10 service.cc:156]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1755365356.182044      10 service.cc:156]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def get_strategy():\n    \"\"\"\n    Detects and returns the best TensorFlow distribution strategy:\n    - TPU if available\n    - MirroredStrategy for GPU(s)\n    - Default strategy for CPU\n    \"\"\"\n    try:\n        # Try to detect a TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu= 'local')  # auto-detects TPU\n        print(\"Running on TPU:\", tpu.master())\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(\"Using TPU strategy:\", type(strategy).__name__)\n    except (ValueError, tf.errors.NotFoundError):\n        try:\n            # If TPU is not found, try GPU\n            physical_devices = tf.config.list_physical_devices('GPU')\n            if physical_devices:\n                strategy = tf.distribute.MirroredStrategy()\n                print(\"Using GPU strategy:\", type(strategy).__name__)\n            else:\n                strategy = tf.distribute.get_strategy()  # default CPU\n                print(\"No TPU/GPU found. Using default strategy:\", type(strategy).__name__)\n        except Exception as e:\n            print(\"Failed to initialize GPU strategy:\", e)\n            strategy = tf.distribute.get_strategy()\n            print(\"Using fallback strategy:\", type(strategy).__name__)\n\n    print(\"REPLICAS:\", strategy.num_replicas_in_sync)\n    return strategy\n\n\n# Call this function once at the beginning of your script\nstrategy = get_strategy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:29:16.211709Z","iopub.execute_input":"2025-08-16T17:29:16.211970Z","iopub.status.idle":"2025-08-16T17:29:21.093780Z","shell.execute_reply.started":"2025-08-16T17:29:16.211945Z","shell.execute_reply":"2025-08-16T17:29:21.088934Z"}},"outputs":[{"name":"stdout","text":"Running on TPU: \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nUsing TPU strategy: TPUStrategyV2\nREPLICAS: 8\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"REPLICAS:\", strategy.num_replicas_in_sync)\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:02.431647Z","iopub.execute_input":"2025-08-16T17:33:02.431996Z","iopub.status.idle":"2025-08-16T17:33:02.443067Z","shell.execute_reply.started":"2025-08-16T17:33:02.431967Z","shell.execute_reply":"2025-08-16T17:33:02.437045Z"}},"outputs":[{"name":"stdout","text":"REPLICAS: 8\nTensorFlow version: 2.18.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## âš¡ 4. Mixed Precision Training\n\nTo further optimize our training process, we'll enable mixed precision. This technique uses a combination of 16-bit and 32-bit floating-point types during training. It can significantly speed up computations and reduce memory usage, especially on modern GPUs and TPUs, without a substantial loss in model accuracy.","metadata":{}},{"cell_type":"code","source":"# List all physical devices labeled as 'GPU'\ngpus = tf.config.list_physical_devices('GPU')\n\nif gpus:\n    print(f\"âœ… GPU detected: {gpus}\")\n    mixed_precision.set_global_policy('mixed_float16')\nelse:\n    print(\"âŒ No GPU found. Using CPU instead.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T16:13:23.875992Z","iopub.execute_input":"2025-08-16T16:13:23.876271Z","iopub.status.idle":"2025-08-16T16:13:23.885848Z","shell.execute_reply.started":"2025-08-16T16:13:23.876247Z","shell.execute_reply":"2025-08-16T16:13:23.881394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ“‚ 5. Data Loading and Exploration\n\nNow, let's define the paths to our data files. We need the directory containing the training images, the CSV file with image labels, and the JSON file that maps numerical labels to their corresponding disease names.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'\nTRAIN_DIR = os.path.join(DATA_DIR, 'train_images')\nCSV_PATH = os.path.join(DATA_DIR, 'train.csv')\nLABEL_PATH = os.path.join(DATA_DIR, 'label_num_to_disease_map.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:08.512011Z","iopub.execute_input":"2025-08-16T17:33:08.512342Z","iopub.status.idle":"2025-08-16T17:33:08.521018Z","shell.execute_reply.started":"2025-08-16T17:33:08.512317Z","shell.execute_reply":"2025-08-16T17:33:08.517828Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Let's have a look at some of the image file names\nprint(os.listdir(TRAIN_DIR)[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:24:52.770704Z","iopub.execute_input":"2025-08-16T11:24:52.770970Z","iopub.status.idle":"2025-08-16T11:24:53.022889Z","shell.execute_reply.started":"2025-08-16T11:24:52.770948Z","shell.execute_reply":"2025-08-16T11:24:53.022230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read one image to check its dimensions\nimg_files = os.listdir(TRAIN_DIR)\nimg_path = os.path.join(TRAIN_DIR, img_files[0])\nimg = cv2.imread(img_path)\nprint(img.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:24:54.783668Z","iopub.execute_input":"2025-08-16T11:24:54.783913Z","iopub.status.idle":"2025-08-16T11:24:54.826143Z","shell.execute_reply.started":"2025-08-16T11:24:54.783896Z","shell.execute_reply":"2025-08-16T11:24:54.825530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the label-to-disease name mapping from the JSON file\nwith open(LABEL_PATH, 'r') as f:\n    label_map = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:24:56.508861Z","iopub.execute_input":"2025-08-16T11:24:56.509152Z","iopub.status.idle":"2025-08-16T11:24:56.517502Z","shell.execute_reply.started":"2025-08-16T11:24:56.509131Z","shell.execute_reply":"2025-08-16T11:24:56.516814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading the Training DataFrame\nWe load the `train.csv` file into a pandas DataFrame. This file contains the `image_id` and its corresponding `label`. We'll convert the labels to string type for easier handling.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(CSV_PATH)\n# Convert the 'label' column to string type for consistency\ndf['label'] = df['label'].astype(str)\nprint(df.tail())\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:25:05.182901Z","iopub.execute_input":"2025-08-16T11:25:05.183140Z","iopub.status.idle":"2025-08-16T11:25:05.224919Z","shell.execute_reply.started":"2025-08-16T11:25:05.183122Z","shell.execute_reply":"2025-08-16T11:25:05.224238Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analyzing Class Distribution\nUnderstanding the distribution of classes is a vital step in any classification problem. It helps us identify if there is a class imbalance, which can affect the model's performance. A significant imbalance might require special techniques like class weighting or over/under-sampling.","metadata":{}},{"cell_type":"code","source":"# Get the count of images for each class\ndf['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:25:09.004102Z","iopub.execute_input":"2025-08-16T11:25:09.004387Z","iopub.status.idle":"2025-08-16T11:25:09.016481Z","shell.execute_reply.started":"2025-08-16T11:25:09.004363Z","shell.execute_reply":"2025-08-16T11:25:09.015777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the class distribution using a bar chart\nplt.figure(figsize=(25,8))\nax=sns.countplot(x=df[\"label\"],palette=\"viridis\",order=df['label'].value_counts().index)\n# Add labels on top of the bars for clarity\nfor p in ax.containers:\n    ax.bar_label(p, fontsize=20, color='black', padding=5);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:25:09.567332Z","iopub.execute_input":"2025-08-16T11:25:09.567634Z","iopub.status.idle":"2025-08-16T11:25:09.846869Z","shell.execute_reply.started":"2025-08-16T11:25:09.567612Z","shell.execute_reply":"2025-08-16T11:25:09.846078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Sample Images\nTo get a better feel for the dataset, let's visualize one sample image from each of the five categories. This helps us understand the visual characteristics of healthy leaves and leaves affected by different diseases.","metadata":{}},{"cell_type":"code","source":"def show_images_from_each_category(df, label_map, rows= 1, images_per_class= 1):\n    classes = df['label'].unique()\n    num_classes = len(classes)\n    columns = images_per_class\n    \n    plt.figure(figsize= (10* rows,10* columns))\n    \n    for i, c in enumerate(classes):\n        # Get random samples for the current class\n        class_samples = df[df['label'] == c].sample(images_per_class)\n        for j ,(_, row) in enumerate(class_samples.iterrows()):\n            # Construct the full image path\n            img_path = os.path.join(TRAIN_DIR, row['image_id'])\n            img = cv2.imread(img_path)\n            # Convert image from BGR (OpenCV default) to RGB for correct display with Matplotlib\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # Get the disease name from the label map\n            label_name = label_map.get(row['label'], row['label'])\n\n            # Create a subplot for each image\n            plt.subplot(num_classes, columns, i* columns + j +1)\n            plt.imshow(img)\n            plt.title(f'{label_name}')\n            plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:25:12.235203Z","iopub.execute_input":"2025-08-16T11:25:12.235770Z","iopub.status.idle":"2025-08-16T11:25:12.241249Z","shell.execute_reply.started":"2025-08-16T11:25:12.235745Z","shell.execute_reply":"2025-08-16T11:25:12.240468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_images_from_each_category(df, label_map, rows= 1, images_per_class= 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T11:25:12.754981Z","iopub.execute_input":"2025-08-16T11:25:12.755260Z","iopub.status.idle":"2025-08-16T11:25:13.406912Z","shell.execute_reply.started":"2025-08-16T11:25:12.755238Z","shell.execute_reply":"2025-08-16T11:25:13.406237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## âš™ï¸ 6. Data Pipeline with TFRecords\n\nFor efficient data handling, especially with large datasets and TPUs, we will use the **TFRecord** format. TFRecord is a binary file format that stores a sequence of protocol buffer messages, which is highly optimized for reading data in TensorFlow.\n\nFirst, we'll locate the directory containing our pre-processed TFRecord files.","metadata":{}},{"cell_type":"code","source":"tfrecord_dir = '/kaggle/input/cassava-leaf-disease-classification/train_tfrecords'\nprint(os.listdir(tfrecord_dir))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:12.489886Z","iopub.execute_input":"2025-08-16T17:33:12.490246Z","iopub.status.idle":"2025-08-16T17:33:12.508203Z","shell.execute_reply.started":"2025-08-16T17:33:12.490218Z","shell.execute_reply":"2025-08-16T17:33:12.503416Z"}},"outputs":[{"name":"stdout","text":"['ld_train14-1338.tfrec', 'ld_train13-1338.tfrec', 'ld_train04-1338.tfrec', 'ld_train01-1338.tfrec', 'ld_train08-1338.tfrec', 'ld_train00-1338.tfrec', 'ld_train10-1338.tfrec', 'ld_train02-1338.tfrec', 'ld_train15-1327.tfrec', 'ld_train03-1338.tfrec', 'ld_train11-1338.tfrec', 'ld_train12-1338.tfrec', 'ld_train06-1338.tfrec', 'ld_train09-1338.tfrec', 'ld_train07-1338.tfrec', 'ld_train05-1338.tfrec']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Configuration Parameters\nHere, we define several key parameters for our data pipeline and model. \n\n- `AUTO`: An instruction for TensorFlow to automatically tune the level of parallelism for data pipeline operations.\n- `IMAGE_SIZE`: The dimensions to which all images will be resized. EfficientNetB3 performs well with larger image sizes.\n- `NUM_CLASSES`: The number of distinct categories in our classification task.\n- `BATCH_SIZE`: The total number of samples processed in one forward/backward pass. We calculate the **global batch size** by multiplying the per-replica batch size by the number of available replicas (TPU cores or GPUs). This ensures that the model sees the same total number of images per step, regardless of the distribution strategy.","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.AUTOTUNE\nIMAGE_SIZE = (512, 512)\nBATCH_SIZE_PER_REPLICA = 8\nNUM_CLASSES = 5\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nprint(f'Global Batch size: {BATCH_SIZE}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:12.787893Z","iopub.execute_input":"2025-08-16T17:33:12.788171Z","iopub.status.idle":"2025-08-16T17:33:12.798570Z","shell.execute_reply.started":"2025-08-16T17:33:12.788147Z","shell.execute_reply":"2025-08-16T17:33:12.793348Z"}},"outputs":[{"name":"stdout","text":"Global Batch size: 64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Decoding TFRecord Examples\nThis function is responsible for parsing a single example from a TFRecord file. It reads the raw byte strings for the image and its target label, decodes the JPEG image, resizes it to our standard `IMAGE_SIZE`, and finally, one-hot encodes the label. One-hot encoding converts the integer label (e.g., 3) into a binary vector (e.g., `[0, 0, 0, 1, 0]`), which is the required format for categorical cross-entropy loss.","metadata":{}},{"cell_type":"code","source":"def decode_example(example):\n    # Define the structure of the features in the TFRecord file\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n    }\n    # Parse the input `example` protocol buffer using the feature description\n    example = tf.io.parse_single_example(example, feature_description)\n    \n    # Decode the JPEG-encoded image string to a tensor\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    # Resize the image to the desired dimensions\n    image = tf.image.resize(image, IMAGE_SIZE)\n    \n    # Get the integer label\n    label_int = example['target']\n    \n    # One-hot encode the label for the model\n    label = tf.one_hot(label_int, NUM_CLASSES)\n    \n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:14.039123Z","iopub.execute_input":"2025-08-16T17:33:14.039381Z","iopub.status.idle":"2025-08-16T17:33:14.051398Z","shell.execute_reply.started":"2025-08-16T17:33:14.039357Z","shell.execute_reply":"2025-08-16T17:33:14.046085Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Preprocessing Input for EfficientNet\nPre-trained models like EfficientNet expect their input data to be preprocessed in a specific way (e.g., pixel values scaled to a certain range). The `preprocess_input` function from `tf.keras.applications.efficientnet` handles this for us, ensuring our images are in the correct format for the model.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input\n\ndef preprocess(image, label):\n    # Apply the specific preprocessing required by the EfficientNet model\n    image = preprocess_input(image)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:14.362486Z","iopub.execute_input":"2025-08-16T17:33:14.362723Z","iopub.status.idle":"2025-08-16T17:33:14.377701Z","shell.execute_reply.started":"2025-08-16T17:33:14.362701Z","shell.execute_reply":"2025-08-16T17:33:14.373535Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### ğŸ¨ Data Augmentation\nData augmentation is a powerful technique to increase the diversity of the training data without collecting new samples. By applying random transformations like rotations, flips, and zooms to the training images, we can make our model more robust and less prone to overfitting. We define a `Sequential` model that will apply these augmentations on the fly during training.","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    # Geometric Transformations\n    tf.keras.layers.RandomRotation(40/ 360), # Randomly rotate images\n    tf.keras.layers.RandomTranslation(0.2, 0.2), # Randomly shift images horizontally and vertically\n    tf.keras.layers.RandomZoom(0.2, 0.2), # Randomly zoom into images\n    tf.keras.layers.RandomFlip('horizontal'), # Randomly flip images horizontally\n    tf.keras.layers.RandomFlip('vertical') # Randomly flip images vertically\n], name=\"data_augmentation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:15.420079Z","iopub.execute_input":"2025-08-16T17:33:15.420437Z","iopub.status.idle":"2025-08-16T17:33:15.448730Z","shell.execute_reply.started":"2025-08-16T17:33:15.420408Z","shell.execute_reply":"2025-08-16T17:33:15.443436Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Define the augmentation function that will be mapped to the dataset\ndef apply_augmentation(image, label):\n    # Keras augmentation layers expect a batch of images.\n    # We add a batch dimension, apply the augmentation, and then remove it.\n    image = tf.expand_dims(image, 0) # Add batch dimension\n    image = data_augmentation(image)\n    image = tf.squeeze(image, 0)      # Remove batch dimension\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:15.733734Z","iopub.execute_input":"2025-08-16T17:33:15.734059Z","iopub.status.idle":"2025-08-16T17:33:15.744800Z","shell.execute_reply.started":"2025-08-16T17:33:15.734032Z","shell.execute_reply":"2025-08-16T17:33:15.740385Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Assembling the Data Pipeline\nThis function brings everything together to create our final `tf.data.Dataset` object. It reads the TFRecord files, decodes and preprocesses the data, and applies augmentation (only to the training set). Key steps include:\n\n- `.with_options(ignore_order)`: Disables deterministic order to improve performance.\n- `.map()`: Applies our decoding, preprocessing, and augmentation functions in parallel.\n- `.shuffle()`: Shuffles the training data to ensure the model doesn't learn from the order of examples.\n- `.batch()`: Groups the data into batches.\n- `.prefetch()`: Prepares subsequent batches while the current one is being processed, which helps to prevent data pipeline bottlenecks.","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, is_training=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False  # for performance\n    # Create a dataset from the TFRecord files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads= AUTO)\n    # Disable deterministic order for better performance\n    dataset = dataset.with_options(ignore_order)\n    # Decode each example in the dataset\n    dataset = dataset.map(decode_example, num_parallel_calls= AUTO)\n    # Preprocess the images for the model\n    dataset = dataset.map(preprocess, num_parallel_calls= AUTO)\n    # Apply augmentations and shuffling only to the training set\n    if is_training:\n        dataset = dataset.shuffle(8196)\n        dataset = dataset.map(apply_augmentation, num_parallel_calls= AUTO)\n    # Batch the dataset and prefetch for performance\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder= True).prefetch(AUTO)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:16.018306Z","iopub.execute_input":"2025-08-16T17:33:16.018706Z","iopub.status.idle":"2025-08-16T17:33:16.032058Z","shell.execute_reply.started":"2025-08-16T17:33:16.018670Z","shell.execute_reply":"2025-08-16T17:33:16.026030Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Creating Training and Validation Datasets\nWe'll split our TFRecord files into a training set and a validation set. A common split is 80% for training and 20% for validation. The validation set is crucial for monitoring the model's performance on unseen data during training and for tuning hyperparameters.","metadata":{}},{"cell_type":"code","source":"# Get a list of all TFRecord files\nall_files = sorted(tf.io.gfile.glob(os.path.join(tfrecord_dir, '*.tfrec')))\n\n# Split the files into training and validation sets (e.g., 80% train, 20% val)\nsplit_index = int(0.8 * len(all_files)) + 1\ntrain_files = all_files[:split_index]\nval_files = all_files[split_index:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:16.949811Z","iopub.execute_input":"2025-08-16T17:33:16.950172Z","iopub.status.idle":"2025-08-16T17:33:16.965951Z","shell.execute_reply.started":"2025-08-16T17:33:16.950141Z","shell.execute_reply":"2025-08-16T17:33:16.961302Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Create the dataset objects using our pipeline function\ntrain_dataset = load_dataset(train_files, is_training=True)\nval_dataset = load_dataset(val_files, is_training=False)\nprint(f'Train and validation set created successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:17.238478Z","iopub.execute_input":"2025-08-16T17:33:17.238729Z","iopub.status.idle":"2025-08-16T17:33:17.607196Z","shell.execute_reply.started":"2025-08-16T17:33:17.238706Z","shell.execute_reply":"2025-08-16T17:33:17.603045Z"}},"outputs":[{"name":"stdout","text":"Train and validation set created successfully!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Calculating Steps Per Epoch\nTo properly train our model, we need to know how many batches of data constitute one full epoch. We calculate `steps_per_epoch` for the training set and `validation_steps` for the validation set. This is done by counting the total number of images in each set and dividing by the global batch size.","metadata":{}},{"cell_type":"code","source":"# Helper function to count the total number of examples in a set of TFRecord files\ndef count_total_examples(tfrecord_files):\n    count = 0\n    for fname in tfrecord_files:\n        count += sum(1 for _ in tf.data.TFRecordDataset(fname))\n    return count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:18.253186Z","iopub.execute_input":"2025-08-16T17:33:18.253523Z","iopub.status.idle":"2025-08-16T17:33:18.264794Z","shell.execute_reply.started":"2025-08-16T17:33:18.253496Z","shell.execute_reply":"2025-08-16T17:33:18.258978Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Count the number of images in the training and validation sets\nnum_train_images = count_total_examples(train_files)\nnum_val_images = count_total_examples(val_files)\n\n# Calculate the number of steps (batches) per epoch\nsteps_per_epoch = num_train_images // BATCH_SIZE\nvalidation_steps = num_val_images // BATCH_SIZE\n\nprint(f'Batch steps on training: {steps_per_epoch}\\nSteps on validation: {validation_steps}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:19.179388Z","iopub.execute_input":"2025-08-16T17:33:19.179730Z","iopub.status.idle":"2025-08-16T17:33:50.882082Z","shell.execute_reply.started":"2025-08-16T17:33:19.179700Z","shell.execute_reply":"2025-08-16T17:33:50.877304Z"}},"outputs":[{"name":"stdout","text":"Batch steps on training: 271\nSteps on validation: 62\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Verifying the Data Pipeline\nLet's inspect a single batch from our training dataset to ensure that the images and labels have the correct shapes and data types. This is a good sanity check before starting the training process.","metadata":{}},{"cell_type":"code","source":"# Take one batch from the training dataset\nfor images, labels in train_dataset.take(1):\n    # Print the shape of the image and label tensors\n    print(\"Image batch shape:\", images.shape)\n    print(\"Label batch shape:\", labels.shape)\n    # Print an example one-hot encoded label\n    print(\"Label example (one-hot):\", labels[0].numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:12:12.656230Z","iopub.execute_input":"2025-08-15T21:12:12.656500Z","iopub.status.idle":"2025-08-15T21:12:23.030536Z","shell.execute_reply.started":"2025-08-15T21:12:12.656481Z","shell.execute_reply":"2025-08-15T21:12:23.029960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ§  7. Model Building with Transfer Learning\n\nWe will now construct our model using **transfer learning**. We'll use **EfficientNetB3**, a powerful and efficient model pre-trained on the large ImageNet dataset. The idea is to leverage the features learned by this model (like edges, textures, and shapes) and adapt them to our specific task of classifying cassava leaf diseases.\n\nOur model architecture will consist of:\n1.  **The Base Model**: The pre-trained `EfficientNetB3` with its top classification layer removed (`include_top=False`). We'll initially freeze its weights so they don't change during the first phase of training.\n2.  **A Custom Classifier Head**: We will add our own layers on top of the base model:\n    - `GlobalAveragePooling2D`: To flatten the feature maps from the base model.\n    - `Dense` layer with `relu` activation: A hidden layer to learn more complex patterns.\n    - `Dropout`: A regularization technique to prevent overfitting.\n    - `Dense` output layer with `softmax` activation: To produce a probability distribution over the 5 classes.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l2\ndef cassava_model(IMAGE_SIZE):\n    \n    # Load the EfficientNetB3 model, pre-trained on ImageNet\n    base_model = tf.keras.applications.EfficientNetB3(\n        weights= 'imagenet',\n        include_top= False, # Do not include the final ImageNet classifier layer\n        input_shape= IMAGE_SIZE + (3,)\n    )\n    \n    # Freeze the weights of the base model. We will only train the new classifier head initially.\n    base_model.trainable = False\n        \n    # Define the model input\n    inputs = tf.keras.layers.Input(shape= IMAGE_SIZE + (3, ))\n    # Pass the inputs through the base model\n    eff = base_model(inputs)\n    # Add our custom classifier head\n    avg = tf.keras.layers.GlobalAveragePooling2D()(eff)\n    fc = tf.keras.layers.Dense(256, activation= 'relu', kernel_regularizer= l2(0.001))(avg)\n    dropout = tf.keras.layers.Dropout(0.3)(fc)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation= 'softmax')(dropout)\n    \n    # Create the final model\n    model = tf.keras.Model(inputs= inputs, outputs= outputs)\n    # Print a summary of the model architecture\n    model.summary()\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:33:50.884465Z","iopub.execute_input":"2025-08-16T17:33:50.885573Z","iopub.status.idle":"2025-08-16T17:33:50.903741Z","shell.execute_reply.started":"2025-08-16T17:33:50.885532Z","shell.execute_reply":"2025-08-16T17:33:50.897973Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## ğŸ‹ï¸ 8. Training Phase 1: Training the Head\n\nIn the first phase of training, we only train the weights of the custom classifier head we added. The weights of the EfficientNetB3 base model remain frozen. This allows the new layers to learn to interpret the features extracted by the base model for our specific dataset without disrupting the valuable pre-trained knowledge.\n\n### Callbacks\nWe will use several callbacks to manage the training process:\n- `ModelCheckpoint`: Saves the model with the best validation loss.\n- `EarlyStopping`: Stops training if the validation loss doesn't improve for a set number of epochs, preventing overfitting.\n- `ReduceLROnPlateau`: Reduces the learning rate if the training plateaus, helping the model to find a better minimum.","metadata":{}},{"cell_type":"code","source":"# Save the best model based on validation loss\ncheckpoint_cb = ModelCheckpoint(\n    '/kaggle/working/initial_model_cassava.keras', # File to save the best model\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min' # We want to minimize loss\n)\n\n# Stop training if validation loss doesn't improve for 3 epochs\nearly_stopping_cb = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True # This is great, it restores the weights from the best epoch\n)\n\n# Reduce learning rate when learning plateaus\nreduce_lr_cb = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=2,\n    min_lr=1e-6\n)\n\ncallbacks = [checkpoint_cb, early_stopping_cb, reduce_lr_cb]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:34:42.241126Z","iopub.execute_input":"2025-08-16T17:34:42.241346Z","iopub.status.idle":"2025-08-16T17:34:42.257169Z","shell.execute_reply.started":"2025-08-16T17:34:42.241324Z","shell.execute_reply":"2025-08-16T17:34:42.252735Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Compiling and Fitting the Model\nWe compile the model within the `strategy.scope()` to ensure it's distributed across the available TPUs/GPUs. We use the `Adam` optimizer, `CategoricalCrossentropy` loss with **label smoothing** (a regularization technique that prevents the model from becoming overconfident), and track `accuracy` as our performance metric. Then, we start the training process.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # Build the model\n    model = cassava_model(IMAGE_SIZE)\n    # Compile the model with optimizer, loss, and metrics\n    model.compile(optimizer = tf.keras.optimizers.Adam(3e-4), \n                  loss= tf.keras.losses.CategoricalCrossentropy(label_smoothing= 0.01), \n                  metrics= ['accuracy'],\n                  steps_per_execution= 32 if isinstance(strategy, tf.distribute.TPUStrategy) else 1)\n\n# Set the number of epochs for this training phase\ninitial_epoch = 15\nprint(\"--- Starting Phase 1: Training Head Classifier ---\")\n# Fit the model to the training data\nhistory = model.fit(train_dataset.repeat(), \n                    validation_data= val_dataset.repeat(),\n                    epochs= initial_epoch,\n                    callbacks= callbacks,\n                    steps_per_epoch= steps_per_epoch,\n                    validation_steps= validation_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:34:44.056207Z","iopub.execute_input":"2025-08-16T17:34:44.056501Z","iopub.status.idle":"2025-08-16T17:49:31.930597Z","shell.execute_reply.started":"2025-08-16T17:34:44.056478Z","shell.execute_reply":"2025-08-16T17:49:31.925281Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1755365684.126507      10 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n\u001b[1m43941136/43941136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1536\u001b[0m)   â”‚    \u001b[38;5;34m10,783,535\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m393,472\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚         \u001b[38;5;34m1,285\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)   â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,472</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,178,292\u001b[0m (42.64 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,178,292</span> (42.64 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m394,757\u001b[0m (1.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">394,757</span> (1.51 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m10,783,535\u001b[0m (41.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> (41.14 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"--- Starting Phase 1: Training Head Classifier ---\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755365725.486279      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:909619968722227493\nI0000 00:00:1755365732.608292     950 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(6135953942165038386), session_name()\nI0000 00:00:1755365765.161761     950 tpu_compile_op_common.cc:245] Compilation of 6135953942165038386 with session name  took 32.553423036s and succeeded\nI0000 00:00:1755365765.290977     950 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(6135953942165038386), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_909619968722227493\", property.function_library_fingerprint = 11382746915819985232, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"1,1,1,3,;1,1,1,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1755365765.291035     950 tpu_compilation_cache_interface.cc:542] After adding entry for key 6135953942165038386 with session_name  cache is 1 entries (135288947 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m257/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m2s\u001b[0m 172ms/step - accuracy: 0.6693 - loss: 1.3046","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755365822.153901      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:15975597113115234847\nI0000 00:00:1755365825.488001     900 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(3891110384407408329), session_name()\nI0000 00:00:1755365835.845282     900 tpu_compile_op_common.cc:245] Compilation of 3891110384407408329 with session name  took 10.357223603s and succeeded\nI0000 00:00:1755365835.865176     900 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(3891110384407408329), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_15975597113115234847\", property.function_library_fingerprint = 3553460296750522317, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"1,1,1,3,;1,1,1,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1755365835.865222     900 tpu_compilation_cache_interface.cc:542] After adding entry for key 3891110384407408329 with session_name  cache is 2 entries (171513964 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 276ms/step - accuracy: 0.6709 - loss: 1.2999 - val_accuracy: 0.7517 - val_loss: 1.0149 - learning_rate: 3.0000e-04\nEpoch 2/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 221ms/step - accuracy: 0.7533 - loss: 1.0053 - val_accuracy: 0.7734 - val_loss: 0.8885 - learning_rate: 3.0000e-04\nEpoch 3/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 265ms/step - accuracy: 0.7551 - loss: 0.9336 - val_accuracy: 0.7581 - val_loss: 0.8708 - learning_rate: 3.0000e-04\nEpoch 4/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 261ms/step - accuracy: 0.7582 - loss: 0.8563 - val_accuracy: 0.7837 - val_loss: 0.7863 - learning_rate: 3.0000e-04\nEpoch 5/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 207ms/step - accuracy: 0.7829 - loss: 0.7594 - val_accuracy: 0.7805 - val_loss: 0.7585 - learning_rate: 3.0000e-04\nEpoch 7/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 227ms/step - accuracy: 0.7853 - loss: 0.7382 - val_accuracy: 0.7783 - val_loss: 0.7356 - learning_rate: 3.0000e-04\nEpoch 8/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 201ms/step - accuracy: 0.7884 - loss: 0.7299 - val_accuracy: 0.7856 - val_loss: 0.7089 - learning_rate: 3.0000e-04\nEpoch 9/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 211ms/step - accuracy: 0.7986 - loss: 0.7155 - val_accuracy: 0.7976 - val_loss: 0.6791 - learning_rate: 3.0000e-04\nEpoch 10/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 207ms/step - accuracy: 0.7981 - loss: 0.6974 - val_accuracy: 0.7605 - val_loss: 0.7508 - learning_rate: 3.0000e-04\nEpoch 11/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 231ms/step - accuracy: 0.7719 - loss: 0.7277 - val_accuracy: 0.7891 - val_loss: 0.6928 - learning_rate: 3.0000e-04\nEpoch 12/15\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 200ms/step - accuracy: 0.7875 - loss: 0.6934 - val_accuracy: 0.7874 - val_loss: 0.6884 - learning_rate: 6.0000e-05\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Check the history object from your training run\nval_loss_history = history.history['val_loss']\nbest_val_loss = min(val_loss_history)\nbest_epoch = val_loss_history.index(best_val_loss) + 1\n\nprint(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch}\")\nprint(f\"Final Validation Loss: {val_loss_history[-1]:.4f} at Epoch {len(val_loss_history)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:50:05.998588Z","iopub.execute_input":"2025-08-16T17:50:05.998964Z","iopub.status.idle":"2025-08-16T17:50:06.010103Z","shell.execute_reply.started":"2025-08-16T17:50:05.998933Z","shell.execute_reply":"2025-08-16T17:50:06.005643Z"}},"outputs":[{"name":"stdout","text":"Lowest Validation Loss: 0.6791 at Epoch 9\nFinal Validation Loss: 0.6884 at Epoch 12\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## ğŸ¨ 9. Training Phase 2: Fine-Tuning\n\nAfter the classifier head has been trained and has converged, we can move to the fine-tuning phase. Here, we **unfreeze** the entire base model (or some of its top layers) and continue training with a **very low learning rate**. \n\nThis allows the model to make small adjustments to the pre-trained weights, adapting them more closely to the specifics of the cassava leaf dataset. Using a low learning rate is critical to avoid destroying the valuable features learned during pre-training.","metadata":{}},{"cell_type":"code","source":"# Define a new set of callbacks for the fine-tuning phase\nfn_checkpoint_cb = ModelCheckpoint(\n    '/kaggle/working/final_model_cassava.keras', # File to save the best model\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min' # We want to minimize loss\n)\n\n# Stop training if validation loss doesn't improve for 4 epochs\nearly_stopping_cb = EarlyStopping(\n    monitor='val_loss',\n    patience=4,\n    restore_best_weights=True # This is great, it restores the weights from the best epoch\n)\n\n# Reduce learning rate when learning plateaus\nreduce_lr_cb = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=2,\n    min_lr=1e-7\n)\n\ncallbacks = [fn_checkpoint_cb, early_stopping_cb, reduce_lr_cb]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:50:07.690778Z","iopub.execute_input":"2025-08-16T17:50:07.691111Z","iopub.status.idle":"2025-08-16T17:50:07.704781Z","shell.execute_reply.started":"2025-08-16T17:50:07.691085Z","shell.execute_reply":"2025-08-16T17:50:07.699102Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"with strategy.scope():\n\n    # Unfreeze the base model to make it trainable\n    base_model = model.get_layer('efficientnetb3')\n    base_model.trainable = True\n\n    # Re-compile the model with a much lower learning rate for fine-tuning\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(3e-5),\n        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),\n        metrics=['accuracy'],\n        steps_per_execution= 32 if isinstance(strategy, tf.distribute.TPUStrategy) else 1\n    )\n\nprint(\"--- Starting Phase 2: Fine tune some last layers ---\")\nfinal_epochs = initial_epoch + 50\n# Continue training the model\nhistory_final = model.fit(\n    train_dataset.repeat(),\n    validation_data= val_dataset.repeat(),\n    epochs= final_epochs,\n    initial_epoch= initial_epoch, # Start from the epoch number where the first phase left off\n    callbacks= callbacks,\n    steps_per_epoch= steps_per_epoch,\n    validation_steps= validation_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-16T18:11:50.057828Z","shell.execute_reply.started":"2025-08-16T17:50:25.707736Z","shell.execute_reply":"2025-08-16T18:11:50.054506Z"}},"outputs":[{"name":"stdout","text":"--- Starting Phase 2: Fine tune some last layers ---\nEpoch 16/65\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755366676.020060      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:5431622165114090587\nI0000 00:00:1755366694.048052     897 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(6786671375194905774), session_name()\nI0000 00:00:1755366756.403020     897 tpu_compile_op_common.cc:245] Compilation of 6786671375194905774 with session name  took 1m2.354913954s and succeeded\nI0000 00:00:1755366756.647023     897 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(6786671375194905774), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_5431622165114090587\", property.function_library_fingerprint = 3399947844805316696, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"1,1,1,3,;1,1,1,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1755366756.647107     897 tpu_compilation_cache_interface.cc:542] After adding entry for key 6786671375194905774 with session_name  cache is 3 entries (479851191 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m225/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - accuracy: 0.7021 - loss: 0.9366 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1755366821.770939      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:11999025309751306794\nI0000 00:00:1755366825.065777     961 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(9406603734987865858), session_name()\nI0000 00:00:1755366835.858490     961 tpu_compile_op_common.cc:245] Compilation of 9406603734987865858 with session name  took 10.790965844s and succeeded\nI0000 00:00:1755366835.877635     961 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(9406603734987865858), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_one_step_on_data_11999025309751306794\", property.function_library_fingerprint = 8070228795681013558, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"1,1,1,3,;1,1,1,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1755366835.877668     961 tpu_compilation_cache_interface.cc:542] After adding entry for key 9406603734987865858 with session_name  cache is 4 entries (516076208 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 215ms/step - accuracy: 0.8178 - loss: 0.6532 - val_accuracy: 0.8328 - val_loss: 0.5920 - learning_rate: 3.0000e-05\nEpoch 18/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 216ms/step - accuracy: 0.8141 - loss: 0.6331 - val_accuracy: 0.8481 - val_loss: 0.5442 - learning_rate: 3.0000e-05\nEpoch 19/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 278ms/step - accuracy: 0.8511 - loss: 0.5449 - val_accuracy: 0.8513 - val_loss: 0.5297 - learning_rate: 3.0000e-05\nEpoch 20/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 297ms/step - accuracy: 0.8392 - loss: 0.5780 - val_accuracy: 0.8572 - val_loss: 0.5141 - learning_rate: 3.0000e-05\nEpoch 21/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 235ms/step - accuracy: 0.8816 - loss: 0.4928 - val_accuracy: 0.8657 - val_loss: 0.4919 - learning_rate: 3.0000e-05\nEpoch 22/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 211ms/step - accuracy: 0.8645 - loss: 0.5222 - val_accuracy: 0.8638 - val_loss: 0.4931 - learning_rate: 3.0000e-05\nEpoch 23/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 242ms/step - accuracy: 0.8789 - loss: 0.4715 - val_accuracy: 0.8777 - val_loss: 0.4761 - learning_rate: 3.0000e-05\nEpoch 24/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 315ms/step - accuracy: 0.8692 - loss: 0.4752 - val_accuracy: 0.8574 - val_loss: 0.4995 - learning_rate: 3.0000e-05\nEpoch 25/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 258ms/step - accuracy: 0.8822 - loss: 0.4513 - val_accuracy: 0.8508 - val_loss: 0.5150 - learning_rate: 3.0000e-05\nEpoch 26/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 267ms/step - accuracy: 0.8863 - loss: 0.4306 - val_accuracy: 0.8708 - val_loss: 0.4733 - learning_rate: 6.0000e-06\nEpoch 27/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 204ms/step - accuracy: 0.8894 - loss: 0.4428 - val_accuracy: 0.8699 - val_loss: 0.4761 - learning_rate: 6.0000e-06\nEpoch 29/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 226ms/step - accuracy: 0.8978 - loss: 0.4162 - val_accuracy: 0.8723 - val_loss: 0.4749 - learning_rate: 6.0000e-06\nEpoch 30/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 199ms/step - accuracy: 0.8915 - loss: 0.4273 - val_accuracy: 0.8701 - val_loss: 0.4717 - learning_rate: 1.2000e-06\nEpoch 31/65\n\u001b[1m271/271\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 196ms/step - accuracy: 0.8821 - loss: 0.4528 - val_accuracy: 0.8721 - val_loss: 0.4726 - learning_rate: 1.2000e-06\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## ğŸ” 10. Inference on Test Set\n\nWith our model trained and fine-tuned, it's time to generate predictions on the unseen test data. We'll first create a data pipeline for the test set, similar to the one we built for training and validation.","metadata":{}},{"cell_type":"code","source":"# This function decodes examples from the test TFRecord files.\n# Note that test files contain an 'image_name' instead of a 'target' label.\ndef decode_test_example(example):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    \n    # Decode and process the image\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    \n    # Return the image and its ID\n    return image, example['image_name']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:12:42.539908Z","iopub.execute_input":"2025-08-16T18:12:42.540222Z","iopub.status.idle":"2025-08-16T18:12:42.551740Z","shell.execute_reply.started":"2025-08-16T18:12:42.540197Z","shell.execute_reply":"2025-08-16T18:12:42.547071Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Create the test dataset pipeline\nTEST_DIR = os.path.join(DATA_DIR, 'test_tfrecords')\ntest_files = tf.io.gfile.glob(os.path.join(TEST_DIR, '*.tfrec'))\ntest_dataset = tf.data.TFRecordDataset(test_files, num_parallel_reads= AUTO)\ntest_dataset = (test_dataset\n                .map(decode_test_example, num_parallel_calls= AUTO)\n                .map(lambda image, image_id: (preprocess_input(image), image_id), num_parallel_calls= AUTO)\n                .batch(BATCH_SIZE)\n                .prefetch(AUTO))\n\nprint('Test dataset created Successfully !')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:12:44.080538Z","iopub.execute_input":"2025-08-16T18:12:44.080834Z","iopub.status.idle":"2025-08-16T18:12:44.192658Z","shell.execute_reply.started":"2025-08-16T18:12:44.080810Z","shell.execute_reply":"2025-08-16T18:12:44.188448Z"}},"outputs":[{"name":"stdout","text":"Test dataset created Successfully !\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### Test-Time Augmentation (TTA)\n\nTo potentially improve our prediction accuracy, we will use **Test-Time Augmentation (TTA)**. This technique involves creating multiple augmented versions of each test image, making a prediction for each version, and then averaging these predictions. This process can lead to more robust and accurate results as it reduces the impact of random variations in the test images.","metadata":{}},{"cell_type":"code","source":"# A new function to apply augmentation to a single image during inference\ndef apply_test_augmentation(image):\n    # Expand dims to make it a batch of 1\n    image = tf.expand_dims(image, 0)\n    # Apply the same data augmentation layers used in training\n    image = data_augmentation(image)\n    # Squeeze to remove the batch dimension\n    image = tf.squeeze(image, 0)\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:12:45.436918Z","iopub.execute_input":"2025-08-16T18:12:45.437257Z","iopub.status.idle":"2025-08-16T18:12:45.448484Z","shell.execute_reply.started":"2025-08-16T18:12:45.437227Z","shell.execute_reply":"2025-08-16T18:12:45.443222Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# A function to get TTA predictions for a single image\ndef tta_predict_image(model, image, num_tta):\n    # Create 'num_tta' augmented versions of the input image\n    augmented_images = tf.stack([apply_test_augmentation(image) for _ in range(num_tta)])\n    \n    # Preprocess the augmented images for the EfficientNet model\n    preprocessed_images = preprocess_input(augmented_images)\n    \n    # Predict on the batch of augmented images\n    predictions = model.predict(preprocessed_images)\n    \n    # Average the predictions across all augmented versions\n    mean_predictions = tf.reduce_mean(predictions, axis=0)\n    \n    return mean_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:12:46.750120Z","iopub.execute_input":"2025-08-16T18:12:46.750494Z","iopub.status.idle":"2025-08-16T18:12:46.763000Z","shell.execute_reply.started":"2025-08-16T18:12:46.750453Z","shell.execute_reply":"2025-08-16T18:12:46.758332Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Iterate through the test dataset and generate TTA predictions for each image\ntta_num_augmentations = 10  # Number of augmented images to create per test image\ntta_predictions = []\ntta_image_names = []\n\nfor images, ids in test_dataset:\n    for i in range(images.shape[0]):\n        # Get a single image and its ID from the batch\n        image = images[i]\n        image_name = ids[i].numpy().decode('utf-8')\n        \n        # Get TTA prediction for this single image\n        mean_preds = tta_predict_image(model, image, tta_num_augmentations)\n        \n        tta_predictions.append(mean_preds)\n        tta_image_names.append(image_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:12:48.020058Z","iopub.execute_input":"2025-08-16T18:12:48.020371Z","iopub.status.idle":"2025-08-16T18:13:07.909123Z","shell.execute_reply.started":"2025-08-16T18:12:48.020345Z","shell.execute_reply":"2025-08-16T18:13:07.904081Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1755367974.376738      10 encapsulate_tpu_computations_pass.cc:266] Subgraph fingerprint:273101623107724625\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755367974.714230      10 meta_optimizer.cc:966] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node functional_1_1/dense_1/BiasAdd/ReadVariableOp.\nW0000 00:00:1755367974.953875      10 loop_optimizer.cc:933] Skipping loop optimization for Merge node with control input: cond/branch_executed/_8\nI0000 00:00:1755367975.548408     892 tpu_compilation_cache_interface.cc:442] TPU host compilation cache miss: cache_key(17656302233664749318), session_name()\nI0000 00:00:1755367987.402992     892 tpu_compile_op_common.cc:245] Compilation of 17656302233664749318 with session name  took 11.854530147s and succeeded\nI0000 00:00:1755367987.424415     892 tpu_compilation_cache_interface.cc:476] TPU host compilation cache: compilation complete for cache_key(17656302233664749318), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_multi_step_on_data_273101623107724625\", property.function_library_fingerprint = 8959996205997954855, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, std::string(property.shapes_prefix) = \"2,512,512,3,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\nI0000 00:00:1755367987.424451     892 tpu_compilation_cache_interface.cc:542] After adding entry for key 17656302233664749318 with session_name  cache is 5 entries (549825110 bytes),  marked for eviction 0 entries (0 bytes).\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19s/step\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## ğŸ“ 11. Create Submission File\n\nFinally, we process our predictions and format them into a `submission.csv` file as required by the competition. We take the `argmax` of our averaged predictions to get the final predicted class label for each image.","metadata":{}},{"cell_type":"code","source":"# Convert the list of predictions to a NumPy array for final processing\ntta_predictions = np.array(tta_predictions)\n# Find the index of the highest probability for each prediction to get the final label\nfinal_tta_labels = tf.argmax(tta_predictions, axis=1)\npred_labels = final_tta_labels.numpy()\n\n# Create a dictionary for the submission DataFrame\nsubmission_dict = {\n    'image_id': tta_image_names,\n    'label': pred_labels\n}\n# Create the submission DataFrame using pandas\nsubmission_df = pd.DataFrame(submission_dict)\nprint('Submission dataframe created successfully !')\nsubmission_df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:13:12.867297Z","iopub.execute_input":"2025-08-16T18:13:12.867679Z","iopub.status.idle":"2025-08-16T18:13:12.906578Z","shell.execute_reply.started":"2025-08-16T18:13:12.867647Z","shell.execute_reply":"2025-08-16T18:13:12.900905Z"}},"outputs":[{"name":"stdout","text":"Submission dataframe created successfully !\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         image_id  label\n0  2216849948.jpg      4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2216849948.jpg</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Save the DataFrame to a CSV file without the index\nsubmission_df.to_csv('/kaggle/working/submission.csv', index= False)\nprint(f'Submission CSV file created successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:13:16.159964Z","iopub.execute_input":"2025-08-16T18:13:16.160288Z","iopub.status.idle":"2025-08-16T18:13:16.180040Z","shell.execute_reply.started":"2025-08-16T18:13:16.160263Z","shell.execute_reply":"2025-08-16T18:13:16.174523Z"}},"outputs":[{"name":"stdout","text":"Submission CSV file created successfully!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}